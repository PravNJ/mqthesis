\chapter{Developing a Framework for Classification}

\section{Key Requirements and Constraints}

It was demonstrated in prior chapters that the classification of emission-line stars must take into account the various spectral morphologies present in DR3. Furthermore, morphological classification based on naked eye observations of spectra cannot scale with the volume of data present in GALAH DR3. While machine learning methods such as t-SNE and autoencoders \cite{traven2017galah}\cite{vcotar2021galah} have been relatively successful in detecting H$\upalpha$ emission spectra, they were both unable to further classify emission-line spectra into classes such as P Cygni and inverse P Cygni.

A classification approach that is sensitive to the meaningful morphological differences between P Cygni, inverse P Cygni and other species is a crucial requirement. For example, feature engineering must take into account the understanding that P Cygni spectra exhibit a red shifted emission peak while the inverse P Cygni spectra exhibit a blue shifted peak and be able to differentiate between classes such as double-peaked emission spectra and emission lines superimposed on absorption.

Given that DR3 does not have labeled samples of P Cygni and inverse P Cygni spectra, a supervised learning approach to classification will not be suitable in this context. This narrows the field of possible technical and scientific approaches to the unsupervised learning domain \cite{hastie2009elements}. Given that prior work has demonstrated that well known unsupervised clustering methods such as k-means clustering failed to cluster and classify emission-line spectra, this work did not consider these and related methods. 

Instead, this work focused on a first principles based approach, with a particular focus on methods that are sensitive to morphological similarities and differences between spectra. More rudimentary methods such as cross-correlation of signals are examples of techniques that could be sensitive to the various emission line morphologies. However, given that a labeled data set of the various classes of emission-line spectra in DR3 does not exist, this line of inquiry was quickly abandoned as this work was unable to create mean spectra capable of convolving against all DR3 spectra for the various potential classes. Instead, this work explored techniques that can create effects similar to pairwise cross correlation, provide a similarity score (or equivalent) with limited human intervention and with limited or no reliance on rule based approaches that have been used in the past \cite{traven2015gaia}. Finally, given that the feature space is of size $\sim$ \num[round-precision=2,round-mode=figures, scientific-notation=true]{2928752091}, the classification approach must be able to overcome the "curse of dimensionality" and be computationally and memory efficient. 

These constraints allowed this work to narrow the search for suitable methods to a field known as time series clustering, more specifically unsupervised time series clustering based on a concept used extensively in signal processing called dynamic time warping (DTW) \cite{kruskal1983overview}. First introduced in 1975 as an algorithm for speech recognition \cite{itakura1975minimum}, DTW has been modified and adapted extensively across various scientific disciplines. It was demonstrated in Chapter 2 that DR3 spectra and indeed all spectra can be modeled as "time series". While an individual spectrum does not contain a time axis, the monotonically increasing wavelength grid serves as the analogue of the time axis. Time series methods such as DTW have been more generally developed in the domain of time series analysis \cite{nielsen2019practical} and can be suitably adapted to stellar spectra. With clustering, this work does not rely on labelled data, but rather takes a data-driven approach that learns from the emission-line morphologies present in the normalised spectra data of DR3 by comparing pairwise distance based similarities between spectra.

\section{Dynamic Time Warping}

Dynamic time warping (DTW) is a time series analysis algorithm that can measure similarity between two temporal sequences. The similarity can then be used to cluster morphologically similar spectra into meaningful groups. These clusters can then be used to classify spectra into distinct classes. DTW is suitable for clustering problems where the morphology of the signal plays a salient role \cite{nielsen2019practical}. The name of the algorithm itself is inspired by the methodology where two signals are warped to align them on the temporal domain. For stellar spectra, this warping takes place in the wavelength domain and thus produces a discrete dynamic wavelength warping effect.

\begin{figure}[h]
\centering
\includegraphics[scale=0.90]{figures/Dynamic_time_warping.png}
\caption{Each point on the wavelength grid is mapped to a point on the opposite spectrum, but there is no requirement that the mapping is one to one. Reproduced from Nielsen \cite{nielsen2019practical}}
\end{figure}

As indicated in the figure above, expansion or contraction of the wavelength axis to find the best alignment ensures that the morphology of the spectrum plays a dominant role in determining similarity. This algorithm is often described as being akin to comparing the visual "shape" of the spectra, rather than focusing on the sequence of how these shapes are formed on the wavelength axis. The implications of this will be discussed in detail in this chapter. The algorithm follows several steps and constraints which are,

\begin{enumerate}
    \item Every point on the spectrum must be matched with at least one point of the other spectrum
    \item The first and last indices of each spectrum must be matched with their counterparts in the other spectrum
    \item The mapping must be such that the wavelength is increasing rather than decreasing i.e. do not match a point on on spectrum to a point on the other spectrum that has "passed" i.e. blue shifted. 
\end{enumerate}

Steps 2 and 3 do not have a significant impact on the DR3 data set being used in this work as they are sampled to the same wavelength and thus are of equal length. There are many possible ways to align two spectra while adhering to these constraints. The algorithm chooses the match that minimises the distance between the spectra. This distance is a cost function and is measured as the sum of the absolute differences between matched points. The absolute difference in this context is the difference between the points' values (wavelength values). Note that it is this distance measure that this research relies on to serve as the basis for clustering similar spectra into classes.

The Pythonic representation of this algorithm is as follows,

\begin{lstlisting}[language=Python]
# Primary function
def distDTW(lambda1, lambda2):
     DTW={}
     for i in range(len(lambda1)):
         DTW[(i, -1)] = np.inf
     for i in range(len(lambda2)):
         DTW[(-1, i)] = np.inf
     DTW[(-1, -1)] = 0
 
# Calculate the optimum i.e. where distance is minimum
     for i in range(len(lambda1)):
         for j in range(len(lambda2)):
             dist = (lambda1[i] - lambda2[j])**2
             DTW[(i, j)] = dist + min(DTW[(i-1, j)],
                                      DTW[(i, j-1)], 
                                      DTW[(i-1, j-1)])
 
 # Return the associated distance between two spectra
     return sqrt(DTW[len(lambda1)-1, len(lambda2)-1])
\end{lstlisting}

Despite the effectiveness of this algorithm, the computational complexity is of order $O(N^2)$. As this presents a significant computational cost and overhead, this research relies on a linear time complexity i.e. $O(N)$ Python language implementation of DTW called \texttt{FastDTW} to compute the optimum distance between spectra and thus the similarity \cite{salvador2007toward}. \texttt{FastDTW} is a linear approximation to the DTW method above and a full discussion is beyond the scope of this thesis. 

In addition to computational complexity, an important consideration when running DTW is available memory. All spectral data must be loaded into memory (RAM) when computing DTW distances. In the case of DR3, this implies holding $\sim$ \num[round-precision=2,round-mode=figures, scientific-notation=true]{2928752091} features in memory. The computational hardware available for this project had a memory capacity of $\sim$ 300 GB. DTW required a memory capacity of at least 1TB for the $\sim$ \num[round-precision=2,round-mode=figures, scientific-notation=true]{2928752091} features. This is a significant amount of data to hold in memory. 

This work presents two strategies to overcome these limitations and reduce computational and memory overheads.

\begin{enumerate}
    \item Reduce the number of features - As explained in a prior chapter, this research will pre-select only the region around H$\upalpha$. DTW will be run on this region only.
    \item Reduce the search space from the entire DR3 data set to a subset which has a higher chance of yielding P Cygni, inverse P Cygni and other emission-line spectra - The existence of H$\upalpha$ emission features is a precursor to the existence of P Cygni and inverse P Cygni features in a spectrum. Thus this work used the H$\upalpha$ emission-line data set from ÄŒotar et al.\cite{vcotar2021galah} for prototyping the technique. This subset with $\sim$ 10,000 candidates includes data from both GALAH and other related surveys. Presumably, this data set only contains H$\upalpha$ emission-line spectra and not typical spectra.
\end{enumerate}

\section{Agglomerative Hierarchical Clustering}

Given a similarity measure, and in the case of DTW, pairwise distance measurements, hierarchical clustering can be used to group similar spectra into clusters. Once a similarity measure (or a dissimilarity measure) has been specified, hierarchical clustering produces a representation in which clusters at each level of the hierarchy are created by merging clusters at the next lower level. At the lowest level, each cluster contains a single observation. At the highest level, there exists a single cluster that contains all of the data \cite{hastie2009elements}. There are two basic paradigms to traverse these levels or "tree", aggolomerative (bottom-up) and divisive (top-down). 

With agglomerative clustering, each spectrum will form a singleton cluster. At each step, the most similar spectra will be merged into a single cluster, producing one less cluster at the next higher level. The similarity between two spectra is defined as the DTW distance between them. Lower distances imply a greater similarity while greater distances indicate a dissimilarity. 

Based on the similarity (and dissimilarity) between individual spectra, a \emph{cluster dissimilarity} can also be defined. Consider two clusters in such a scheme called $A$ and $B$. The dissimilarity between the two clusters $d(A,B)$ is computed from the set of pairwise observation dissimilarities $d_{ij}$, where one member of the pair $i$ is in $A$ and the other $j$ is in $B$. The \emph{complete linkage dissimilarity} between the two clusters is set to be the dissimilarity of the furthest (most dissimilar) pair of spectra

\begin{equation}
    d(A,B) = \max_{\substack{i \in A \\ j \in B}} d_{ij}
\end{equation}

This is also known as the farthest neighbour technique. Other dissimilarity measures such as \emph{single linkage dissimilarity} or nearest neighbour dissimilarity can be defined as

\begin{equation}
    d(A,B) = \min_{\substack{i \in A \\ j \in B}} d_{ij}
\end{equation}

Hierarchical clusters can be visualised using a dendogram. A dendogram is a binary tree that represents the recursive agglomeration (or division). The height of the tree (or a branch) is proportional to the inter-group dissimilarity defined above. 

\begin{figure}[h]
\centering
\includegraphics[scale=0.60]{figures/complete linkage.png}
\caption{Dendograms for the same data set using different measures. Note the longer branch lengths of the complete linked tree that selects for maximum dissimilarity. Reproduced from Hastie et al.\cite{hastie2009elements}}
\end{figure}

This research uses the complete linkage dissimilarity to cluster spectral groups. The justification for using this measure over the single linkage dissimilarity is to force the separation of P Cygni and inverse P Cygni spectra into distinct groups by exploiting the maximum distance between two individual spectra that belong to these groups. Other distance measures such as group average clustering uses the average dissimilarity between the groups. This research does not consider this measure as it may be less accurate in separating P Cygni from inverse P Cygni. A full discussion of the advantages and disadvantages of each approach is beyond the scope of this thesis. This research also relies on the agglomerative paradigm as it is more robust and has been studied extensively in the literature while the divisive paradigm has not been studied extensively \cite{hastie2009elements}.

\subsection{Selecting the Number of Clusters}

Given that the framework proposed above is an entirely unsupervised machine learning approach, it is generally not required that the cluster size be specified in advance. In the absence of a predefined cluster size, the typical approach would require a plot of the dendogram where suitable cuts be made at a required level. However since the tree can theoretically be cut at any level and can have a maximum cluster size equal to the number of samples and a minimum cluster size equal to 1, as will be demonstrated below, a more meaningful and reliable cut can be made with the aide of astrophysical domain knowledge. In the absence of such knowledge this research would have to cut the dendogram at each level, the examine the clusters and subsequently decide on a suitable cluster size. Furthermore, given that the sample size is at least 10,000 spectra, visually inspecting a dendogram for this data set can be challenging. This research proposes the use of prior art to determine a cluster size in advance, thus eliminating the requirement to visually inspect a complex dendogram of the order of thousands of branches. 

Classes of H$\upalpha$ emission-line spectra have been found using manual methods and as such prior work can provide some guidance regarding the number of clusters. Reipurth et al. in particular proposed the existence of seven morphological groups which include P Cygni and inverse P Cygni as well as five other H$\upalpha$ emission-line spectral classes \cite{reipurth1996hupalpha}. If the cluster size is set only to two, there is a significant risk that other morphologies will be included in the P Cygni cluster and inverse P Cygni cluster thus leading to erroneously classified/labelled clusters. Thus based on the prior art, this research takes cluster sizes between six and ten to be a suitable range. The number of samples this research will subject to automated clustering will be significantly higher than any previous prior art, especially the manual classification approaches detailed in prior chapters. The justification to use a higher number of clusters such as ten is to account for the possibility of additional morphological classes that may have been missed in the prior art during manual observation. 

\section{The Results}

The results of the DTW distance calculation can be visualised as a distance cost plot. The figure below presents the pairwise distances for 6977 samples from DR3 that are present in the ÄŒotar et al. data set. Note that the plot resembles a triangular matrix with the diagonal representing a value of zero for the self distance of a spectrum. Lower values indicate higher similarity while higher distance values indicate lower similarity (or greater dissimilarity). Zooming in to a region reveals further structure concerning similar and dissimilar spectra. The self similar, zero distance diagonal is clearly visible as well as the triangular nature of the plot.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.60]{figures/dtw cotar.png}
\caption{Pairwise DTW distances for spectral samples in ÄŒotar et al. which are also in DR3}
\end{figure}

This distance matrix was used as the basis for complete linkage aggolomerative hierarchical clustering. Cluster size was varied between seven and ten. A mean silhouette score was calculated for each cluster size. This score can then be used as a selection criterion for the cluster size. Silhouette scores can range between -1 and 1. Negative scores indicate that samples may be assigned to the wrong cluster. Values extremely close to zero indicate that clusters may overlap. The best achievable value is 1, although this is rarely achieved in practical unsupervised clustering problems of this nature. Thus the silhouette score is a measure of the efficacy of the clustering process.

\begin{figure}[t]
\centering
\includegraphics[scale=0.60]{figures/dtw cotar zoomed.png}
\caption{Pairwise DTW distances for spectral samples in ÄŒotar et al. which are also in DR3 (zoomed)}
\end{figure}

In order to calculate the mean silhouette score, silhouette coefficients for all samples must be calculated as follows. Compute the mean intra-cluster distances given by $a$ and then compute the mean nearest-cluster distance $b$ for each sample. The sample silhouette coefficient is then given by,

\begin{equation}
\frac{(b-a)}{\max_{}(a,b)}
\end{equation}

Once the coefficient for each sample is calculated, the mean silhouette score for the entire sample is computed. The following table summarises the scores for each cluster size.


\begin{table}[]
\begin{center}
\begin{tabular}{|c|c|}
\hline
\textbf{Cluster Size} & \textbf{Silhouette Score} \\ \hline
6                     & 0.2904                    \\ \hline
7                     & 0.3033                    \\ \hline
8                     & 0.3005                    \\ \hline
9                     & 0.3092                    \\ \hline
10                    & 0.3044                    \\ \hline
\end{tabular}
\caption{Silhouette score comparison for each cluster size.}
\label{table:Silhouette Score}
\end{center}
\end{table}

At face value it appears that a cluster size of nine is the optimum cluster size given that this produces the largest silhouette score. However upon closer examination it was concluded that a cluster size of ten is more suitable under the essential constraint that P Cygni and inverse P Cygni spectra should be adequately separated from other clusters. Thus, the spectra that belong to each cluster for both nine clusters and ten clusters were plotted. For nine clusters, while a P Cygni only cluster was discovered, a cluster containing inverse P Cygni spectra was not discovered. However for a cluster size of ten, two clear clusters of P Cygni and inverse P Cygni spectra were discovered.

\begin{figure}[h]
\centering
\includegraphics[scale=0.45]{figures/pcygni.png}
\caption{102 P Cygni spectra identified using clustering.}
\end{figure}

The inverse P Cygni cluster contains an outlier. This will be discussed further in subsequent chapters.

\begin{figure}[h]
\centering
\includegraphics[scale=0.45]{figures/inverse p cygni.png}
\caption{63 Inverse P Cygni spectra identified using clustering.}
\end{figure}

Eight other clusters with various emission line morphologies such as double peak were also discovered. A full gallery of these classes is presented in the appendix. Given the silhouette score, it is possible that some P Cygni and inverse P Cygni spectra may have been mis-classified and included in other classes. This can be addressed by further sub-clustering and classification of spectra via a second pass of the scheme above. However this research did not progress with multiple passes of this method due to time constraints. 

A sub classification of the P Cygni and inverse P Cygni classes discovered in this process is presented in Chapter 5 where these results are compared to t-SNE. The merits and demerits of this approach compared to t-SNE is discussed extensively in this chapter. 

\section{Line Fitting}

The P Cygni and inverse P Cygni spectra thus classified were modeled using a double Gaussian with an offset for one of the Gaussian functions \cite{traven2015gaia}\cite{traven2017galah}\cite{zhang2021catalog}. This mixture model with two Gaussians can be used to fit the line profile of P Cygni and inverse P Cygni spectra. Given that there are 102 P Cygni spectra that require fitting, this research adopts a semi-automated method where the initial guesses for the model parameters were driven by the data, notably the use of minimum and maximum local flux values to set both the amplitude and mid value of each peak and trough. Further details can be found in the appendix.

This research utilised the popular Python based numerical optimisation package, \texttt{scipy.optimize} to generate fitted models for the P Cygni and inverse P Cygni spectra using non-linear least squares.

The Gaussian mixture model is defined as, 

\begin{equation}
    f(x) = \frac{A}{\sigma_1\sqrt{2\pi}} 
  \exp\left( -\frac{1}{2}\left(\frac{x-\mu_1}{\sigma_1}\right)^{\!2}\,\right) + \frac{B}{\sigma_2\sqrt{2\pi}} 
  \exp\left( -\frac{1}{2}\left(\frac{x-\mu_2}{\sigma_2}\right)^{\!2}\,\right) + C
\end{equation}


This seven parameter model contains an offset parameter $C$ to account for the inverted Gaussian required to model the absorption trough/line of the P Cygni and inverse P Cygni spectra. The parameters A and B are to account for the respective amplitudes of the emission and absorption lines. The uncertainties of this fit can be modelled more accurately within a Bayesian framework and the use of MCMC estimation. However this is beyond the scope of this work \cite{hogg2010data}.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.45]{figures/p cugni fitted.png}
\caption{A Gaussian mixture model fit of one of the identified P Cygni spectra.}
\end{figure}

The process can be repeated for each P Cygni spectrum discovered by clustering. Below are a few examples of these fitted models. 

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.45]{figures/p cygni fitted 2.png}
\caption{A Gaussian mixture model fit for sobject ID 140117002101365. }
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.45]{figures/p cygni fitted 3.png}
\caption{A Gaussian mixture model fit for sobject ID 140414005101301. }
\end{figure}

This process was also extended to the inverse P Cygni spectra.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.45]{figures/inverse p cygni 1.png}
\caption{A Gaussian mixture model fit of one of the identified inverse P Cygni spectrum. }
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.45]{figures/inverse p cygni fitted 2.png}
\caption{A Gaussian mixture model fit for sobject ID 140414005101301 (inverse P Cygni spectrum). }
\end{figure}










